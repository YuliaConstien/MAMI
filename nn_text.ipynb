{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "aLuv_VLO5yEb"
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from sklearn.metrics import f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# converting return value from list to string\n",
    "\n",
    "def clean_text(text ):\n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation}\n",
    "    delete_dict[' '] = ' '\n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    \n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))])\n",
    "\n",
    "    return text2.lower()\n",
    "  #print('cleaned:'+text1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "z3XYFdpz5yEe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Train data--------\n",
      "1    4982\n",
      "0    4964\n",
      "Name: misogynous, dtype: int64\n",
      "9946\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "9946"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload and process Dataset\n",
    "train_data= pd.read_csv(\"../data/training/TRAINING/training.csv\", sep=\"\\t\") # Put your path to training.csv here\n",
    "train_data.dropna(axis = 0, how ='any',inplace=True)\n",
    "train_data['Num_words_text'] = train_data['Text Transcription'].apply(lambda x:len(str(x).split()))\n",
    "train_data['text'] = train_data['Text Transcription']\n",
    "mask = train_data['Num_words_text'] >2\n",
    "train_data = train_data[mask]\n",
    "\n",
    "print('-------Train data--------')\n",
    "print(train_data['misogynous'].value_counts())\n",
    "print(len(train_data))\n",
    "print('-------------------------')\n",
    "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
    "\n",
    "\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(clean_text)\n",
    "mask.sum()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xyX4UgMh5yEi",
    "outputId": "551ee7be-113b-4c9c-d779-25f952d4ea5e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  \\\n1     roses are red violets are blue you dont say ye...   \n2     breaking news russia releases photo donald tru...   \n3                               man seeking woman ignad   \n4     explaining the deep lore jrr tolkeins world ar...   \n5     pictophle app straight white malle starts talk...   \n...                                                 ...   \n9995           waiting for the end the covid imgflipcom   \n9996                  smart women are around imgflipcom   \n9997        good girls are behind the corner imgflipcom   \n9998                        cooking for wife imgflipcom   \n9999  listen tomorrow will monday imgflipcom from yo...   \n\n                                     Text Transcription  \n1     ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...  \n2     BREAKING NEWS: Russia releases photo of DONALD...  \n3                          MAN SEEKING WOMAN Ignad 18 O  \n4     Me explaining the deep lore of. J.R.R. Tolkein...  \n5     PICTOPHLE APP *Straight white malle starts tal...  \n...                                                 ...  \n9995      WAITING FOR THE END OF THE COVID  imgflip.com  \n9996                SMART WOMEN ARE AROUND  imgflip.com  \n9997      GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com  \n9998                   COOKING FOR MY WIFE  imgflip.com  \n9999  LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...  \n\n[9946 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Text Transcription</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>roses are red violets are blue you dont say ye...</td>\n      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>breaking news russia releases photo donald tru...</td>\n      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>man seeking woman ignad</td>\n      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>explaining the deep lore jrr tolkeins world ar...</td>\n      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>pictophle app straight white malle starts talk...</td>\n      <td>PICTOPHLE APP *Straight white malle starts tal...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>waiting for the end the covid imgflipcom</td>\n      <td>WAITING FOR THE END OF THE COVID  imgflip.com</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>smart women are around imgflipcom</td>\n      <td>SMART WOMEN ARE AROUND  imgflip.com</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>good girls are behind the corner imgflipcom</td>\n      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>cooking for wife imgflipcom</td>\n      <td>COOKING FOR MY WIFE  imgflip.com</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>listen tomorrow will monday imgflipcom from yo...</td>\n      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n    </tr>\n  </tbody>\n</table>\n<p>9946 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text after pre-processing\n",
    "\n",
    "train_data[['text', 'Text Transcription']]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P8iygDbo5yEj",
    "outputId": "48d9d7b0-969d-4d7f-d149-a70080b952c9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Test data--------\n",
      "0    56\n",
      "1    43\n",
      "Name: misogynous, dtype: int64\n",
      "99\n",
      "-------------------------\n",
      "Train Max Sentence Length :269\n",
      "Test Max Sentence Length :30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data['label'] = train_data['misogynous']\n",
    "\n",
    "test_data= pd.read_csv(\"../data/trial/Users/fersiniel/Desktop/MAMI - TO LABEL/TRIAL DATASET/trial.csv\",sep=\"\\t\") #put you path to trial.csv here\n",
    "test_data.dropna(axis = 0, how ='any',inplace=True)\n",
    "test_data['Num_words_text'] = test_data['Text Transcription'].apply(lambda x:len(str(x).split()))\n",
    "\n",
    "max_test_sentence_length  = test_data['Num_words_text'].max()\n",
    "\n",
    "mask = test_data['Num_words_text'] >2\n",
    "test_data = test_data[mask]\n",
    "\n",
    "print('-------Test data--------')\n",
    "print(test_data['misogynous'].value_counts())\n",
    "print(len(test_data))\n",
    "print('-------------------------')\n",
    "\n",
    "test_data['text']=test_data['Text Transcription']\n",
    "test_data['text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "\n",
    "test_data['label'] = test_data['misogynous']\n",
    "print('Train Max Sentence Length :'+str(max_train_sentence_length))\n",
    "print('Test Max Sentence Length :'+str(max_test_sentence_length))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IeqAfeXW5yEk",
    "outputId": "2b23a184-597d-40c9-b9db-cd4169620db6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#merge\n",
    "train_data=train_data.append(test_data,ignore_index=True)"
   ],
   "metadata": {
    "id": "3S9NoU4c0trm"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56453/2497328984.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_data=train_data.append(test_data,ignore_index=True)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:8036\n",
      "Class distributionCounter({1: 4020, 0: 4016})\n",
      "Valid data len:2009\n",
      "Class distributionCounter({1: 1005, 0: 1004})\n",
      "Test data len:99\n",
      "Class distributionCounter({0: 56, 1: 43})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid= train_test_split(train_data['text'].tolist(),\\\n",
    "                                                      train_data['label'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['label'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(Y_train)))\n",
    "\n",
    "\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(Y_valid)))\n",
    "\n",
    "print('Test data len:'+str(len(test_data['text'].tolist())))\n",
    "print('Class distribution'+ str(Counter(test_data['label'].tolist())))\n",
    "\n",
    "\n",
    "train_dat =list(zip(Y_train,X_train))\n",
    "valid_dat =list(zip(Y_valid,X_valid))\n",
    "test_dat=list(zip(test_data['label'].tolist(),test_data['text'].tolist()))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LDKNEmfb5yEn",
    "outputId": "549aa519-58cc-44c8-88ab-9e2f9f8ec36b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = train_dat\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "s_e3uEjD5yEp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Zpq2u8s-5yEq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[133, 0, 1, 0, 3052]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xVpaG9k25yEr",
    "outputId": "88f05568-5ba1-4081-973e-105078d14f00",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QhR7cXDW5yEs",
    "outputId": "7529d2be-03f8-4fef-bd9a-01ca2c7e0b66",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Create batces\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "#train_iter =train_dat\n",
    "#dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nHk_IneN5yEt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,64)\n",
    "        self.fc2 = nn.Linear(64,16)\n",
    "        self.fc3 = nn.Linear(16, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "XpOVw9lX5yEu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#Initialize model\n",
    "\n",
    "train_iter1 = train_dat\n",
    "num_class = len(set([label for (label, text) in train_iter1]))\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 128\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "EdCoATN-5yEv",
    "outputId": "85e9de9c-e2fa-4e4c-b19d-acfc142e95b9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/  503 batches | accuracy    0.509\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  3.28s | valid accuracy    0.500 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/  503 batches | accuracy    0.654\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  3.14s | valid accuracy    0.712 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/  503 batches | accuracy    0.728\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.50s | valid accuracy    0.734 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/  503 batches | accuracy    0.781\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  1.95s | valid accuracy    0.732 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/  503 batches | accuracy    0.845\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  1.94s | valid accuracy    0.749 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/  503 batches | accuracy    0.866\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.05s | valid accuracy    0.752 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/  503 batches | accuracy    0.879\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.02s | valid accuracy    0.749 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/  503 batches | accuracy    0.895\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  1.98s | valid accuracy    0.748 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/  503 batches | accuracy    0.896\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.04s | valid accuracy    0.748 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/  503 batches | accuracy    0.895\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.04s | valid accuracy    0.749 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "EPOCHS = 10 # epoch\n",
    "LR =10  # learning rate\n",
    "BATCH_SIZE = 16 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter2 = train_dat\n",
    "test_iter2 =test_dat\n",
    "valid_iter2= valid_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_iter2, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_iter2, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "val_acc = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    val_acc.append(accu_val)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PACjjNCp5yEw",
    "outputId": "84cee377-3c34-4b85-febe-93679591daa4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc)\n",
    "plt.title('Test Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2_31naK_5yEy",
    "outputId": "90992e29-ca1e-45eb-a678-e4773f24558e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.899\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "HJFlJeO45yEz",
    "outputId": "195ffaa0-9176-44fe-aa75-cb93ae9363b9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "     file_name                                 Text Transcription  \\\n0    15236.jpg  FACEBOOK SINGLES GROUPS BELIKE WHEN A NEW WOMA...   \n1    15805.jpg    SO, IF YOU'RE A FEMINIST HOW CAN YOU EAT DAIRY?   \n2    16254.jpg         WHEN A CUTE GIRL LEFT YOUR MESSAGE ON SEEN   \n3    16191.jpg  Photographing something you want to show every...   \n4    15952.jpg  HEY BABE CAN YOU MAKE ME A SANDWICH? Hey babe ...   \n..         ...                                                ...   \n995  15591.jpg  IT'S NOT YOUR FAULT You didn't design the dres...   \n996  15049.jpg  THINK ABOUT HOW MUCH BETTER HER SKIN IS BREATH...   \n997  15363.jpg  THE STEREOTYPES ARE TRUE F SHE DOES HAVE A TIG...   \n998  15199.jpg  DRAWS NAKED PICTURES OF BLACK WOMEN 00 0000 GE...   \n999  15853.jpg  \"You work too much.\" Me: OOL I want to be rich...   \n\n                                                  text  \n0    facebook singles groups belike when new woman ...  \n1                 youre feminist how can you eat dairy  \n2                when cute girl left your message seen  \n3    photographing something you want show everyone...  \n4    hey babe can you make sandwich hey babe can yo...  \n..                                                 ...  \n995  its not your fault you didnt design the dressy...  \n996  think about how much better her skin breathing...  \n997  the stereotypes are true she does have tight p...  \n998  draws naked pictures black women gets mad when...  \n999                    you work too much ool want rich  \n\n[1000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>Text Transcription</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15236.jpg</td>\n      <td>FACEBOOK SINGLES GROUPS BELIKE WHEN A NEW WOMA...</td>\n      <td>facebook singles groups belike when new woman ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15805.jpg</td>\n      <td>SO, IF YOU'RE A FEMINIST HOW CAN YOU EAT DAIRY?</td>\n      <td>youre feminist how can you eat dairy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16254.jpg</td>\n      <td>WHEN A CUTE GIRL LEFT YOUR MESSAGE ON SEEN</td>\n      <td>when cute girl left your message seen</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16191.jpg</td>\n      <td>Photographing something you want to show every...</td>\n      <td>photographing something you want show everyone...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15952.jpg</td>\n      <td>HEY BABE CAN YOU MAKE ME A SANDWICH? Hey babe ...</td>\n      <td>hey babe can you make sandwich hey babe can yo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>15591.jpg</td>\n      <td>IT'S NOT YOUR FAULT You didn't design the dres...</td>\n      <td>its not your fault you didnt design the dressy...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>15049.jpg</td>\n      <td>THINK ABOUT HOW MUCH BETTER HER SKIN IS BREATH...</td>\n      <td>think about how much better her skin breathing...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>15363.jpg</td>\n      <td>THE STEREOTYPES ARE TRUE F SHE DOES HAVE A TIG...</td>\n      <td>the stereotypes are true she does have tight p...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>15199.jpg</td>\n      <td>DRAWS NAKED PICTURES OF BLACK WOMEN 00 0000 GE...</td>\n      <td>draws naked pictures black women gets mad when...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>15853.jpg</td>\n      <td>\"You work too much.\" Me: OOL I want to be rich...</td>\n      <td>you work too much ool want rich</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../data/test/Test.csv\",sep=\"\\t\") # Put your path to Test.csv here\n",
    "test_data['text'] = test_data['Text Transcription'].apply(clean_text)\n",
    "test_data"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aLqGZpdI5yE1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "a1d71cd4-e189-4636-b488-c25542dfe77b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "0 5\n",
      "1 6\n",
      "0 7\n",
      "1 8\n",
      "1 9\n",
      "0 10\n",
      "1 11\n",
      "1 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "1 18\n",
      "0 19\n",
      "0 20\n",
      "1 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "1 25\n",
      "1 26\n",
      "1 27\n",
      "1 28\n",
      "1 29\n",
      "1 30\n",
      "0 31\n",
      "0 32\n",
      "0 33\n",
      "0 34\n",
      "1 35\n",
      "0 36\n",
      "0 37\n",
      "0 38\n",
      "1 39\n",
      "1 40\n",
      "0 41\n",
      "1 42\n",
      "0 43\n",
      "1 44\n",
      "1 45\n",
      "1 46\n",
      "0 47\n",
      "1 48\n",
      "1 49\n",
      "0 50\n",
      "1 51\n",
      "1 52\n",
      "1 53\n",
      "1 54\n",
      "0 55\n",
      "1 56\n",
      "0 57\n",
      "0 58\n",
      "1 59\n",
      "0 60\n",
      "1 61\n",
      "1 62\n",
      "1 63\n",
      "1 64\n",
      "1 65\n",
      "1 66\n",
      "1 67\n",
      "1 68\n",
      "0 69\n",
      "1 70\n",
      "1 71\n",
      "1 72\n",
      "1 73\n",
      "1 74\n",
      "0 75\n",
      "1 76\n",
      "1 77\n",
      "0 78\n",
      "1 79\n",
      "1 80\n",
      "1 81\n",
      "0 82\n",
      "0 83\n",
      "1 84\n",
      "1 85\n",
      "1 86\n",
      "0 87\n",
      "1 88\n",
      "1 89\n",
      "1 90\n",
      "0 91\n",
      "0 92\n",
      "0 93\n",
      "1 94\n",
      "0 95\n",
      "0 96\n",
      "1 97\n",
      "1 98\n",
      "0 99\n",
      "1 100\n",
      "1 101\n",
      "1 102\n",
      "1 103\n",
      "0 104\n",
      "1 105\n",
      "1 106\n",
      "1 107\n",
      "0 108\n",
      "1 109\n",
      "1 110\n",
      "1 111\n",
      "1 112\n",
      "1 113\n",
      "0 114\n",
      "1 115\n",
      "1 116\n",
      "0 117\n",
      "1 118\n",
      "0 119\n",
      "1 120\n",
      "1 121\n",
      "1 122\n",
      "0 123\n",
      "0 124\n",
      "1 125\n",
      "1 126\n",
      "1 127\n",
      "1 128\n",
      "0 129\n",
      "1 130\n",
      "0 131\n",
      "1 132\n",
      "1 133\n",
      "1 134\n",
      "1 135\n",
      "1 136\n",
      "0 137\n",
      "1 138\n",
      "0 139\n",
      "0 140\n",
      "0 141\n",
      "0 142\n",
      "1 143\n",
      "0 144\n",
      "0 145\n",
      "0 146\n",
      "0 147\n",
      "1 148\n",
      "1 149\n",
      "1 150\n",
      "0 151\n",
      "1 152\n",
      "1 153\n",
      "1 154\n",
      "1 155\n",
      "0 156\n",
      "0 157\n",
      "0 158\n",
      "0 159\n",
      "1 160\n",
      "0 161\n",
      "0 162\n",
      "0 163\n",
      "0 164\n",
      "0 165\n",
      "1 166\n",
      "0 167\n",
      "1 168\n",
      "0 169\n",
      "1 170\n",
      "1 171\n",
      "0 172\n",
      "1 173\n",
      "1 174\n",
      "1 175\n",
      "1 176\n",
      "1 177\n",
      "0 178\n",
      "1 179\n",
      "0 180\n",
      "0 181\n",
      "0 182\n",
      "0 183\n",
      "1 184\n",
      "1 185\n",
      "0 186\n",
      "0 187\n",
      "1 188\n",
      "0 189\n",
      "1 190\n",
      "0 191\n",
      "1 192\n",
      "0 193\n",
      "1 194\n",
      "1 195\n",
      "1 196\n",
      "1 197\n",
      "0 198\n",
      "1 199\n",
      "1 200\n",
      "1 201\n",
      "0 202\n",
      "0 203\n",
      "1 204\n",
      "0 205\n",
      "0 206\n",
      "0 207\n",
      "1 208\n",
      "1 209\n",
      "1 210\n",
      "0 211\n",
      "1 212\n",
      "0 213\n",
      "1 214\n",
      "0 215\n",
      "1 216\n",
      "0 217\n",
      "1 218\n",
      "1 219\n",
      "1 220\n",
      "0 221\n",
      "0 222\n",
      "1 223\n",
      "0 224\n",
      "0 225\n",
      "1 226\n",
      "1 227\n",
      "0 228\n",
      "0 229\n",
      "0 230\n",
      "0 231\n",
      "0 232\n",
      "1 233\n",
      "0 234\n",
      "0 235\n",
      "0 236\n",
      "0 237\n",
      "0 238\n",
      "0 239\n",
      "0 240\n",
      "1 241\n",
      "1 242\n",
      "0 243\n",
      "0 244\n",
      "1 245\n",
      "1 246\n",
      "0 247\n",
      "0 248\n",
      "0 249\n",
      "0 250\n",
      "0 251\n",
      "0 252\n",
      "1 253\n",
      "0 254\n",
      "1 255\n",
      "1 256\n",
      "1 257\n",
      "1 258\n",
      "1 259\n",
      "1 260\n",
      "1 261\n",
      "0 262\n",
      "1 263\n",
      "0 264\n",
      "1 265\n",
      "0 266\n",
      "1 267\n",
      "1 268\n",
      "1 269\n",
      "1 270\n",
      "1 271\n",
      "0 272\n",
      "0 273\n",
      "0 274\n",
      "1 275\n",
      "0 276\n",
      "1 277\n",
      "1 278\n",
      "1 279\n",
      "0 280\n",
      "1 281\n",
      "0 282\n",
      "0 283\n",
      "1 284\n",
      "0 285\n",
      "0 286\n",
      "0 287\n",
      "0 288\n",
      "0 289\n",
      "1 290\n",
      "0 291\n",
      "1 292\n",
      "1 293\n",
      "0 294\n",
      "0 295\n",
      "0 296\n",
      "1 297\n",
      "0 298\n",
      "0 299\n",
      "0 300\n",
      "1 301\n",
      "0 302\n",
      "1 303\n",
      "0 304\n",
      "1 305\n",
      "0 306\n",
      "1 307\n",
      "1 308\n",
      "0 309\n",
      "0 310\n",
      "1 311\n",
      "0 312\n",
      "1 313\n",
      "1 314\n",
      "1 315\n",
      "1 316\n",
      "1 317\n",
      "0 318\n",
      "0 319\n",
      "0 320\n",
      "0 321\n",
      "0 322\n",
      "0 323\n",
      "0 324\n",
      "0 325\n",
      "1 326\n",
      "1 327\n",
      "1 328\n",
      "1 329\n",
      "1 330\n",
      "0 331\n",
      "1 332\n",
      "1 333\n",
      "1 334\n",
      "1 335\n",
      "1 336\n",
      "0 337\n",
      "0 338\n",
      "0 339\n",
      "1 340\n",
      "0 341\n",
      "1 342\n",
      "1 343\n",
      "1 344\n",
      "0 345\n",
      "0 346\n",
      "1 347\n",
      "0 348\n",
      "1 349\n",
      "0 350\n",
      "1 351\n",
      "1 352\n",
      "0 353\n",
      "1 354\n",
      "0 355\n",
      "0 356\n",
      "1 357\n",
      "0 358\n",
      "0 359\n",
      "1 360\n",
      "0 361\n",
      "1 362\n",
      "0 363\n",
      "1 364\n",
      "0 365\n",
      "0 366\n",
      "0 367\n",
      "1 368\n",
      "1 369\n",
      "1 370\n",
      "1 371\n",
      "1 372\n",
      "1 373\n",
      "0 374\n",
      "0 375\n",
      "0 376\n",
      "1 377\n",
      "0 378\n",
      "1 379\n",
      "1 380\n",
      "0 381\n",
      "0 382\n",
      "0 383\n",
      "0 384\n",
      "0 385\n",
      "0 386\n",
      "1 387\n",
      "0 388\n",
      "0 389\n",
      "1 390\n",
      "0 391\n",
      "1 392\n",
      "0 393\n",
      "1 394\n",
      "1 395\n",
      "1 396\n",
      "0 397\n",
      "0 398\n",
      "0 399\n",
      "1 400\n",
      "0 401\n",
      "1 402\n",
      "1 403\n",
      "1 404\n",
      "1 405\n",
      "1 406\n",
      "0 407\n",
      "1 408\n",
      "0 409\n",
      "0 410\n",
      "1 411\n",
      "1 412\n",
      "1 413\n",
      "0 414\n",
      "0 415\n",
      "Opa!\n",
      "0 417\n",
      "1 418\n",
      "1 419\n",
      "0 420\n",
      "1 421\n",
      "1 422\n",
      "1 423\n",
      "0 424\n",
      "1 425\n",
      "0 426\n",
      "1 427\n",
      "1 428\n",
      "0 429\n",
      "0 430\n",
      "0 431\n",
      "1 432\n",
      "0 433\n",
      "0 434\n",
      "0 435\n",
      "0 436\n",
      "0 437\n",
      "1 438\n",
      "0 439\n",
      "0 440\n",
      "1 441\n",
      "0 442\n",
      "1 443\n",
      "0 444\n",
      "0 445\n",
      "1 446\n",
      "1 447\n",
      "0 448\n",
      "1 449\n",
      "0 450\n",
      "1 451\n",
      "0 452\n",
      "1 453\n",
      "0 454\n",
      "1 455\n",
      "1 456\n",
      "0 457\n",
      "0 458\n",
      "1 459\n",
      "1 460\n",
      "0 461\n",
      "1 462\n",
      "1 463\n",
      "0 464\n",
      "1 465\n",
      "1 466\n",
      "0 467\n",
      "1 468\n",
      "1 469\n",
      "1 470\n",
      "0 471\n",
      "1 472\n",
      "1 473\n",
      "1 474\n",
      "0 475\n",
      "0 476\n",
      "1 477\n",
      "1 478\n",
      "0 479\n",
      "0 480\n",
      "0 481\n",
      "1 482\n",
      "1 483\n",
      "1 484\n",
      "1 485\n",
      "1 486\n",
      "0 487\n",
      "0 488\n",
      "0 489\n",
      "1 490\n",
      "1 491\n",
      "1 492\n",
      "1 493\n",
      "1 494\n",
      "1 495\n",
      "1 496\n",
      "1 497\n",
      "0 498\n",
      "1 499\n",
      "0 500\n",
      "1 501\n",
      "1 502\n",
      "1 503\n",
      "1 504\n",
      "0 505\n",
      "0 506\n",
      "0 507\n",
      "1 508\n",
      "1 509\n",
      "1 510\n",
      "1 511\n",
      "1 512\n",
      "1 513\n",
      "1 514\n",
      "1 515\n",
      "1 516\n",
      "1 517\n",
      "0 518\n",
      "1 519\n",
      "1 520\n",
      "0 521\n",
      "0 522\n",
      "0 523\n",
      "0 524\n",
      "0 525\n",
      "0 526\n",
      "1 527\n",
      "0 528\n",
      "1 529\n",
      "0 530\n",
      "0 531\n",
      "0 532\n",
      "1 533\n",
      "1 534\n",
      "0 535\n",
      "1 536\n",
      "1 537\n",
      "1 538\n",
      "1 539\n",
      "0 540\n",
      "0 541\n",
      "1 542\n",
      "0 543\n",
      "0 544\n",
      "1 545\n",
      "1 546\n",
      "0 547\n",
      "1 548\n",
      "0 549\n",
      "1 550\n",
      "1 551\n",
      "0 552\n",
      "1 553\n",
      "0 554\n",
      "0 555\n",
      "1 556\n",
      "0 557\n",
      "0 558\n",
      "1 559\n",
      "0 560\n",
      "1 561\n",
      "0 562\n",
      "0 563\n",
      "1 564\n",
      "0 565\n",
      "1 566\n",
      "0 567\n",
      "0 568\n",
      "1 569\n",
      "1 570\n",
      "0 571\n",
      "0 572\n",
      "1 573\n",
      "0 574\n",
      "0 575\n",
      "1 576\n",
      "1 577\n",
      "1 578\n",
      "1 579\n",
      "1 580\n",
      "1 581\n",
      "1 582\n",
      "1 583\n",
      "1 584\n",
      "1 585\n",
      "0 586\n",
      "1 587\n",
      "1 588\n",
      "0 589\n",
      "0 590\n",
      "0 591\n",
      "1 592\n",
      "1 593\n",
      "1 594\n",
      "1 595\n",
      "1 596\n",
      "1 597\n",
      "1 598\n",
      "1 599\n",
      "1 600\n",
      "1 601\n",
      "0 602\n",
      "1 603\n",
      "1 604\n",
      "0 605\n",
      "1 606\n",
      "0 607\n",
      "1 608\n",
      "1 609\n",
      "1 610\n",
      "1 611\n",
      "1 612\n",
      "1 613\n",
      "0 614\n",
      "1 615\n",
      "1 616\n",
      "1 617\n",
      "1 618\n",
      "1 619\n",
      "0 620\n",
      "1 621\n",
      "1 622\n",
      "0 623\n",
      "1 624\n",
      "1 625\n",
      "1 626\n",
      "1 627\n",
      "1 628\n",
      "1 629\n",
      "1 630\n",
      "1 631\n",
      "1 632\n",
      "1 633\n",
      "1 634\n",
      "0 635\n",
      "1 636\n",
      "1 637\n",
      "1 638\n",
      "1 639\n",
      "0 640\n",
      "0 641\n",
      "1 642\n",
      "1 643\n",
      "0 644\n",
      "0 645\n",
      "1 646\n",
      "1 647\n",
      "1 648\n",
      "1 649\n",
      "1 650\n",
      "0 651\n",
      "0 652\n",
      "1 653\n",
      "0 654\n",
      "0 655\n",
      "1 656\n",
      "1 657\n",
      "0 658\n",
      "1 659\n",
      "0 660\n",
      "0 661\n",
      "1 662\n",
      "1 663\n",
      "1 664\n",
      "1 665\n",
      "0 666\n",
      "1 667\n",
      "1 668\n",
      "1 669\n",
      "1 670\n",
      "1 671\n",
      "1 672\n",
      "1 673\n",
      "1 674\n",
      "0 675\n",
      "0 676\n",
      "0 677\n",
      "0 678\n",
      "0 679\n",
      "1 680\n",
      "0 681\n",
      "1 682\n",
      "1 683\n",
      "0 684\n",
      "1 685\n",
      "0 686\n",
      "1 687\n",
      "0 688\n",
      "1 689\n",
      "0 690\n",
      "0 691\n",
      "0 692\n",
      "1 693\n",
      "1 694\n",
      "1 695\n",
      "1 696\n",
      "0 697\n",
      "0 698\n",
      "1 699\n",
      "1 700\n",
      "1 701\n",
      "1 702\n",
      "1 703\n",
      "1 704\n",
      "1 705\n",
      "1 706\n",
      "1 707\n",
      "1 708\n",
      "1 709\n",
      "0 710\n",
      "1 711\n",
      "1 712\n",
      "1 713\n",
      "1 714\n",
      "1 715\n",
      "1 716\n",
      "1 717\n",
      "0 718\n",
      "0 719\n",
      "0 720\n",
      "0 721\n",
      "0 722\n",
      "1 723\n",
      "1 724\n",
      "0 725\n",
      "0 726\n",
      "0 727\n",
      "1 728\n",
      "1 729\n",
      "1 730\n",
      "0 731\n",
      "0 732\n",
      "1 733\n",
      "1 734\n",
      "1 735\n",
      "0 736\n",
      "0 737\n",
      "1 738\n",
      "1 739\n",
      "1 740\n",
      "0 741\n",
      "0 742\n",
      "1 743\n",
      "1 744\n",
      "1 745\n",
      "0 746\n",
      "0 747\n",
      "0 748\n",
      "0 749\n",
      "1 750\n",
      "1 751\n",
      "1 752\n",
      "1 753\n",
      "1 754\n",
      "1 755\n",
      "0 756\n",
      "0 757\n",
      "0 758\n",
      "0 759\n",
      "1 760\n",
      "1 761\n",
      "0 762\n",
      "0 763\n",
      "0 764\n",
      "0 765\n",
      "1 766\n",
      "1 767\n",
      "0 768\n",
      "1 769\n",
      "0 770\n",
      "0 771\n",
      "0 772\n",
      "1 773\n",
      "0 774\n",
      "1 775\n",
      "1 776\n",
      "1 777\n",
      "1 778\n",
      "0 779\n",
      "1 780\n",
      "1 781\n",
      "1 782\n",
      "1 783\n",
      "1 784\n",
      "0 785\n",
      "1 786\n",
      "0 787\n",
      "0 788\n",
      "1 789\n",
      "1 790\n",
      "0 791\n",
      "0 792\n",
      "0 793\n",
      "0 794\n",
      "0 795\n",
      "1 796\n",
      "1 797\n",
      "1 798\n",
      "0 799\n",
      "1 800\n",
      "1 801\n",
      "1 802\n",
      "0 803\n",
      "0 804\n",
      "0 805\n",
      "0 806\n",
      "0 807\n",
      "0 808\n",
      "0 809\n",
      "0 810\n",
      "1 811\n",
      "0 812\n",
      "0 813\n",
      "1 814\n",
      "0 815\n",
      "1 816\n",
      "0 817\n",
      "1 818\n",
      "0 819\n",
      "1 820\n",
      "1 821\n",
      "0 822\n",
      "0 823\n",
      "1 824\n",
      "0 825\n",
      "1 826\n",
      "0 827\n",
      "1 828\n",
      "1 829\n",
      "0 830\n",
      "1 831\n",
      "0 832\n",
      "1 833\n",
      "1 834\n",
      "0 835\n",
      "1 836\n",
      "0 837\n",
      "0 838\n",
      "0 839\n",
      "1 840\n",
      "0 841\n",
      "1 842\n",
      "1 843\n",
      "1 844\n",
      "1 845\n",
      "0 846\n",
      "1 847\n",
      "1 848\n",
      "0 849\n",
      "0 850\n",
      "1 851\n",
      "0 852\n",
      "1 853\n",
      "0 854\n",
      "0 855\n",
      "0 856\n",
      "1 857\n",
      "1 858\n",
      "1 859\n",
      "0 860\n",
      "1 861\n",
      "1 862\n",
      "1 863\n",
      "1 864\n",
      "0 865\n",
      "0 866\n",
      "1 867\n",
      "1 868\n",
      "0 869\n",
      "1 870\n",
      "1 871\n",
      "0 872\n",
      "0 873\n",
      "0 874\n",
      "1 875\n",
      "1 876\n",
      "0 877\n",
      "1 878\n",
      "0 879\n",
      "1 880\n",
      "0 881\n",
      "0 882\n",
      "1 883\n",
      "1 884\n",
      "0 885\n",
      "1 886\n",
      "1 887\n",
      "0 888\n",
      "1 889\n",
      "1 890\n",
      "1 891\n",
      "1 892\n",
      "0 893\n",
      "0 894\n",
      "1 895\n",
      "0 896\n",
      "0 897\n",
      "1 898\n",
      "1 899\n",
      "1 900\n",
      "1 901\n",
      "1 902\n",
      "0 903\n",
      "0 904\n",
      "1 905\n",
      "1 906\n",
      "0 907\n",
      "1 908\n",
      "0 909\n",
      "1 910\n",
      "1 911\n",
      "0 912\n",
      "0 913\n",
      "1 914\n",
      "1 915\n",
      "0 916\n",
      "1 917\n",
      "1 918\n",
      "0 919\n",
      "1 920\n",
      "0 921\n",
      "1 922\n",
      "0 923\n",
      "1 924\n",
      "0 925\n",
      "1 926\n",
      "1 927\n",
      "1 928\n",
      "1 929\n",
      "0 930\n",
      "1 931\n",
      "1 932\n",
      "1 933\n",
      "1 934\n",
      "1 935\n",
      "1 936\n",
      "0 937\n",
      "0 938\n",
      "1 939\n",
      "1 940\n",
      "1 941\n",
      "0 942\n",
      "1 943\n",
      "1 944\n",
      "1 945\n",
      "0 946\n",
      "0 947\n",
      "1 948\n",
      "0 949\n",
      "1 950\n",
      "1 951\n",
      "1 952\n",
      "1 953\n",
      "1 954\n",
      "1 955\n",
      "1 956\n",
      "0 957\n",
      "1 958\n",
      "0 959\n",
      "1 960\n",
      "0 961\n",
      "0 962\n",
      "0 963\n",
      "1 964\n",
      "1 965\n",
      "1 966\n",
      "1 967\n",
      "1 968\n",
      "0 969\n",
      "0 970\n",
      "1 971\n",
      "0 972\n",
      "1 973\n",
      "0 974\n",
      "1 975\n",
      "0 976\n",
      "0 977\n",
      "1 978\n",
      "1 979\n",
      "0 980\n",
      "1 981\n",
      "0 982\n",
      "1 983\n",
      "1 984\n",
      "0 985\n",
      "1 986\n",
      "0 987\n",
      "1 988\n",
      "0 989\n",
      "0 990\n",
      "0 991\n",
      "1 992\n",
      "1 993\n",
      "1 994\n",
      "0 995\n",
      "1 996\n",
      "1 997\n",
      "1 998\n",
      "0 999\n"
     ]
    }
   ],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "pred = []\n",
    "errors = 0\n",
    "for i, text in enumerate(test_data['text']):\n",
    "    try:\n",
    "        ex_text_str = text\n",
    "        model = model.to(\"cpu\")\n",
    "        print(predict(ex_text_str, text_pipeline),i)\n",
    "        pred.append(predict(ex_text_str, text_pipeline))\n",
    "    except RuntimeError:\n",
    "        print('Opa!')\n",
    "        errors += 1\n",
    "        pred.append(0)\n",
    "\n",
    "pred = np.array(pred)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "17VreIgU5yE1",
    "outputId": "182bc707-c686-477c-8fde-fd9961e83c4b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "arrayInput = pred\n",
    "fileName = \"text_output.pkl\"\n",
    "fileObject = open(fileName, 'wb')\n",
    "pkl.dump(arrayInput, fileObject)\n",
    "fileObject.close()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NoKYweIZ5yE2"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "nn_text.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}